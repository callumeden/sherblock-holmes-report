\chapter{Entity Tagging}\label{section-entity-tagging}
As discussed in background section \ref{background-tag-collection}, it is possible to associate public addresses with known entities, such as exchanges, pools and services. Wallet Explorer hosts a wealth of data mapping entities (e.g exchanges, pools, services etc) to the public addresses they are known to operate under \cite{RefWorks:doc:5c4b26f3e4b0ea619646d513}. Therefore, this objective is to collect address tagging information from Wallet Explorer and to associate it with the historical Bitcoin Blockchain data we have previously collected.

\section{Retrieving Wallet Data}\label{design:fetch-entity-data}
Wallet Explorer does not provide functionality for users to download the data they serve. The data can only be viewed by navigating to each wallet, then their addresses, then through a series of paginated tables providing the addresses for that entity.
\\\\
Our solution to this problem was to build a web scraping program which navigates the site and builds a mapping for each wallet (entity) to all of its addresses, which can then be written to some output file for mapping against the stored bitcoin data at a later stage. 

\subsection{Building the scraper}
We used a popular open-source web-scraping framework scrapy for implementing the scraper application. We generate a new web crawler by extending the framework's 'CrawlSpider' class. In this class, we define the bounds of the crawl and define methods to parse and extract the data for each page it visits. For example, we created rules to allow the crawler to navigate to links with 'addresses' in the URL and then defined how to extract the wallet name,  each of the addresses from the table and how to follow links to paginate the table in order to view all addresses for that wallet. 
\\\\
However, when we initially ran the scraper application, we continuously received 403 (Forbidden) error responses for the majority of requests.  After investigation, we discovered the likely cause of this was due to anti-scraping measures many websites employ. In order to circumvent these anti-scraping measures, we experimented with a number of approaches recommended by scrapy's documentation titled 'Avoid getting banned'; these included rotating user agents, disabling cookies, use download delays or use a pool of rotating IP's. 

\subsubsection{Implementing delays} 
This approach worked; we were able to visit many pages without being served a 403 response as before, however, this took a very long time. A delay of more than 2 seconds was required to circumvent the website's scraping detection such that the delay solution works; therefore, this would be an inefficient solution for scraping thousands of pages containing the required data on the walletexplorer site. 

\subsubsection{Using rotating proxies}
We integrated into the scraper project scrapoxy which allows us to hide the scraper behind a cloud provider (in this instance AWS). Scrapoxy creates a pool of proxies and routes all requests through the pool of proxies. We configured scrapoxy to use a personal Amazon EC2 account and was able to scale up to 20 EC2 instances when performing the scraping (to enable greater concurrency, but limited at 20 because the account is restricted to 20 instances).  Through trial and error, we were able to configure the number of concurrent requests per proxy to 5, which further maximises concurrency without encountering error responses due to anti-scraping measures. 

\section{Results}
Using 20 EC2 instances, the scrape completed on 14-04-2019 at 10:07:42 successfully after 1 day, 19 hours and 12 minutes and received 240,411 HTTP responses. This amounted to generating a 920MB data file containing entity to address mappings. Due to the concurrency of the retrieval process, distinct threads may have fetched different pages of data for each wallet. To simplify the scraping tool and so that it runs as fast as possible, the thread will write that wallets information as a new entry in the JSON. However, this will lead to multiple duplicate keys, such as in the example below. 
\begin{lstlisting}
[
{"wallet": "wallet1", "addresses": ["add1", "add2"]},
{"wallet": "wallet2", "addresses": ["add3"]},
{"wallet": "wallet1", "addresses": ["add4", "add5"]}
]
\end{lstlisting}

whereas we would like the desired format to be of the format:

\begin{lstlisting}
{
"wallet1": ["add1", "add2", "add4", "add5],
"wallet2": ["add3"]
}
\end{lstlisting}
Thankfully the size of the input file was still small enough to load into memory, making this transformation simpler. We created a simple script which iterated through the input file and generated a new dictionary containing wallet names as unique keys and concatenated lists of addresses from each entry matching the wallet name. We then wrote this out in JSON format to a new file. 

\section{Performing the address matching}\label{address-matching-trie}
From step \ref{design:fetch-entity-data} we have a JSON mapping each entity type to a list of addresses. From importing the Bitcoin Blockchain into CSV format, and de-duplicating addresses, we have a CSV containing all distinct addresses to have ever been seen on the chain. We now have the task of creating a new \texttt{HAS\_ENTITY} relationship for each address in the CSV that occurs in the entity mapping JSON file. If we were to take a naive approach here, where we compare each address from the blockchain against each address we have a mapping for, we would have an algorithm which performs $\mathcal{O}(nm)$ address matches, where $n$ is the number of addresses found on the blockchain and $m$ the number of addresses we have an entity mapping for. 
\\\\
We took a more efficient approach and used a Trie data structure. The Trie data more efficiently stores Bitcoin addresses data since it only needs to store once the common prefixes of the millions of addresses. The primary benefit, though, is for performing the address matching. For each address in the blockchain, we only need to walk, at most, the height of the tree, so we only require $\mathcal{O}(n)$ address matches. We were able to simply implement this approach using Tries by using Google's pygtrie library. 